{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn TF-IDF vs SPLADE Classifier Comparison\n",
    "\n",
    "This notebook compares two text classification approaches on the AG News dataset:\n",
    "\n",
    "1. **sklearn TF-IDF + LogisticRegression** - Traditional sparse bag-of-words approach\n",
    "2. **SPLADE Neural Classifier** - Neural sparse representations with interpretability\n",
    "\n",
    "We evaluate both on:\n",
    "- Accuracy and F1 scores\n",
    "- Training and inference time\n",
    "- Sparsity of representations\n",
    "- Interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import time\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom datasets import load_dataset\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n\nfrom src.models import SPLADEClassifier\n\n# Set random seeds for reproducibility\nimport torch\nnp.random.seed(42)\ntorch.manual_seed(42)\n\n# Display settings\nplt.rcParams['figure.facecolor'] = 'white'\nplt.rcParams['axes.grid'] = True\nplt.rcParams['grid.alpha'] = 0.3\n%matplotlib inline\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load AG News Dataset\n",
    "\n",
    "AG News is a 4-class news topic classification dataset with 120,000 training samples.\n",
    "\n",
    "**Classes:**\n",
    "- 0: World\n",
    "- 1: Sports\n",
    "- 2: Business\n",
    "- 3: Sci/Tech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load AG News from HuggingFace\n",
    "print(\"Loading AG News dataset...\")\n",
    "dataset = load_dataset(\"ag_news\")\n",
    "\n",
    "print(f\"\\nDataset structure:\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Extract texts and labels (convert to lists for compatibility)\nCLASS_NAMES = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n\ntrain_texts = list(dataset[\"train\"][\"text\"])\ntrain_labels = list(dataset[\"train\"][\"label\"])\n\ntest_texts = list(dataset[\"test\"][\"text\"])\ntest_labels = list(dataset[\"test\"][\"label\"])\n\nprint(f\"Training samples: {len(train_texts):,}\")\nprint(f\"Test samples: {len(test_texts):,}\")\nprint(f\"Number of classes: {len(CLASS_NAMES)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution\n",
    "train_label_counts = pd.Series(train_labels).value_counts().sort_index()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Bar chart of class distribution\n",
    "axes[0].bar(CLASS_NAMES, train_label_counts.values, color=['#e74c3c', '#3498db', '#2ecc71', '#9b59b6'])\n",
    "axes[0].set_ylabel('Number of samples')\n",
    "axes[0].set_title('Training Set Class Distribution')\n",
    "for i, v in enumerate(train_label_counts.values):\n",
    "    axes[0].text(i, v + 500, f'{v:,}', ha='center')\n",
    "\n",
    "# Text length distribution\n",
    "text_lengths = [len(t.split()) for t in train_texts[:5000]]  # Sample for speed\n",
    "axes[1].hist(text_lengths, bins=50, color='steelblue', edgecolor='white')\n",
    "axes[1].set_xlabel('Number of words')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Text Length Distribution (sample)')\n",
    "axes[1].axvline(np.median(text_lengths), color='red', linestyle='--', label=f'Median: {np.median(text_lengths):.0f}')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nText length statistics (words):\")\n",
    "print(f\"  Min: {np.min(text_lengths)}, Max: {np.max(text_lengths)}, Median: {np.median(text_lengths):.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample examples from each class\n",
    "print(\"Sample examples from each class:\\n\")\n",
    "for class_idx, class_name in enumerate(CLASS_NAMES):\n",
    "    # Find first example of this class\n",
    "    for i, label in enumerate(train_labels):\n",
    "        if label == class_idx:\n",
    "            print(f\"=== {class_name} ===\")\n",
    "            print(train_texts[i][:300] + \"...\")\n",
    "            print()\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. sklearn TF-IDF + LogisticRegression Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training sklearn TF-IDF + LogisticRegression...\\n\")\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf_start = time.time()\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=30000,  # Match SPLADE vocab size roughly\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    max_df=0.95,\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(train_texts)\n",
    "X_test_tfidf = vectorizer.transform(test_texts)\n",
    "\n",
    "tfidf_time = time.time() - tfidf_start\n",
    "print(f\"TF-IDF vectorization: {tfidf_time:.2f}s\")\n",
    "print(f\"TF-IDF vocabulary size: {len(vectorizer.vocabulary_):,}\")\n",
    "print(f\"TF-IDF matrix shape: {X_train_tfidf.shape}\")\n",
    "print(f\"TF-IDF sparsity: {(1 - X_train_tfidf.nnz / (X_train_tfidf.shape[0] * X_train_tfidf.shape[1])) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression\n",
    "lr_start = time.time()\n",
    "\n",
    "lr_clf = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    solver='lbfgs',\n",
    "    multi_class='multinomial',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "lr_clf.fit(X_train_tfidf, train_labels)\n",
    "\n",
    "sklearn_train_time = tfidf_time + (time.time() - lr_start)\n",
    "print(f\"\\nTotal sklearn training time: {sklearn_train_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate sklearn model\n",
    "sklearn_inference_start = time.time()\n",
    "sklearn_preds = lr_clf.predict(X_test_tfidf)\n",
    "sklearn_inference_time = time.time() - sklearn_inference_start\n",
    "\n",
    "sklearn_accuracy = accuracy_score(test_labels, sklearn_preds)\n",
    "sklearn_f1 = f1_score(test_labels, sklearn_preds, average='macro')\n",
    "\n",
    "print(\"sklearn TF-IDF + LogisticRegression Results:\")\n",
    "print(f\"  Accuracy: {sklearn_accuracy:.4f}\")\n",
    "print(f\"  F1 (macro): {sklearn_f1:.4f}\")\n",
    "print(f\"  Inference time: {sklearn_inference_time:.2f}s\")\n",
    "print()\n",
    "print(classification_report(test_labels, sklearn_preds, target_names=CLASS_NAMES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SPLADE Neural Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SPLADE classifier for 4-class classification\n",
    "print(\"Initializing SPLADE classifier...\")\n",
    "\n",
    "splade_clf = SPLADEClassifier(\n",
    "    model_name=\"distilbert-base-uncased\",\n",
    "    max_length=128,\n",
    "    batch_size=32,\n",
    "    learning_rate=2e-5,\n",
    "    flops_lambda=1e-4,\n",
    "    num_labels=4,\n",
    "    class_names=CLASS_NAMES,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"Device: {splade_clf.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SPLADE model\n",
    "# Note: For faster demo, you can use a subset of the data\n",
    "# For full comparison, use all training data (takes ~1-2 hours on GPU)\n",
    "\n",
    "# Subsample for faster training (optional - comment out for full training)\n",
    "TRAIN_SUBSET = 10000  # Use None for full dataset\n",
    "\n",
    "if TRAIN_SUBSET:\n",
    "    indices = np.random.choice(len(train_texts), TRAIN_SUBSET, replace=False)\n",
    "    splade_train_texts = [train_texts[i] for i in indices]\n",
    "    splade_train_labels = [train_labels[i] for i in indices]\n",
    "    print(f\"Training on subset: {len(splade_train_texts):,} samples\")\n",
    "else:\n",
    "    splade_train_texts = train_texts\n",
    "    splade_train_labels = train_labels\n",
    "    print(f\"Training on full dataset: {len(splade_train_texts):,} samples\")\n",
    "\n",
    "splade_train_start = time.time()\n",
    "\n",
    "splade_clf.fit(\n",
    "    splade_train_texts,\n",
    "    splade_train_labels,\n",
    "    epochs=3\n",
    ")\n",
    "\n",
    "splade_train_time = time.time() - splade_train_start\n",
    "print(f\"\\nSPLADE training time: {splade_train_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate SPLADE model\n",
    "print(\"Evaluating SPLADE on test set...\")\n",
    "\n",
    "splade_inference_start = time.time()\n",
    "splade_preds = splade_clf.predict(test_texts)\n",
    "splade_inference_time = time.time() - splade_inference_start\n",
    "\n",
    "splade_accuracy = accuracy_score(test_labels, splade_preds)\n",
    "splade_f1 = f1_score(test_labels, splade_preds, average='macro')\n",
    "\n",
    "print(\"\\nSPLADE Classifier Results:\")\n",
    "print(f\"  Accuracy: {splade_accuracy:.4f}\")\n",
    "print(f\"  F1 (macro): {splade_f1:.4f}\")\n",
    "print(f\"  Inference time: {splade_inference_time:.2f}s\")\n",
    "print()\n",
    "print(classification_report(test_labels, splade_preds, target_names=CLASS_NAMES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute SPLADE sparsity\n",
    "print(\"Computing SPLADE sparsity on sample...\")\n",
    "sample_texts = test_texts[:100]\n",
    "splade_sparsity = splade_clf.get_sparsity(sample_texts)\n",
    "print(f\"SPLADE vector sparsity: {splade_sparsity:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparison & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison table\n",
    "comparison_data = {\n",
    "    'Metric': ['Accuracy', 'F1 (macro)', 'Training Time (s)', 'Inference Time (s)', 'Sparsity (%)'],\n",
    "    'sklearn TF-IDF': [\n",
    "        f\"{sklearn_accuracy:.4f}\",\n",
    "        f\"{sklearn_f1:.4f}\",\n",
    "        f\"{sklearn_train_time:.2f}\",\n",
    "        f\"{sklearn_inference_time:.2f}\",\n",
    "        f\"{(1 - X_test_tfidf.nnz / (X_test_tfidf.shape[0] * X_test_tfidf.shape[1])) * 100:.2f}\"\n",
    "    ],\n",
    "    'SPLADE': [\n",
    "        f\"{splade_accuracy:.4f}\",\n",
    "        f\"{splade_f1:.4f}\",\n",
    "        f\"{splade_train_time:.2f}\",\n",
    "        f\"{splade_inference_time:.2f}\",\n",
    "        f\"{splade_sparsity:.2f}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Side-by-side comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Accuracy comparison\n",
    "models = ['sklearn\\nTF-IDF', 'SPLADE']\n",
    "accuracies = [sklearn_accuracy, splade_accuracy]\n",
    "colors = ['#3498db', '#e74c3c']\n",
    "\n",
    "axes[0].bar(models, accuracies, color=colors)\n",
    "axes[0].set_ylim(0.7, 1.0)\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Accuracy Comparison')\n",
    "for i, v in enumerate(accuracies):\n",
    "    axes[0].text(i, v + 0.01, f'{v:.4f}', ha='center', fontweight='bold')\n",
    "\n",
    "# F1 comparison\n",
    "f1_scores = [sklearn_f1, splade_f1]\n",
    "axes[1].bar(models, f1_scores, color=colors)\n",
    "axes[1].set_ylim(0.7, 1.0)\n",
    "axes[1].set_ylabel('F1 Score (macro)')\n",
    "axes[1].set_title('F1 Score Comparison')\n",
    "for i, v in enumerate(f1_scores):\n",
    "    axes[1].text(i, v + 0.01, f'{v:.4f}', ha='center', fontweight='bold')\n",
    "\n",
    "# Training time comparison\n",
    "train_times = [sklearn_train_time, splade_train_time]\n",
    "axes[2].bar(models, train_times, color=colors)\n",
    "axes[2].set_ylabel('Time (seconds)')\n",
    "axes[2].set_title('Training Time Comparison')\n",
    "for i, v in enumerate(train_times):\n",
    "    axes[2].text(i, v + max(train_times)*0.02, f'{v:.1f}s', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# sklearn confusion matrix\n",
    "sklearn_cm = confusion_matrix(test_labels, sklearn_preds)\n",
    "sns.heatmap(sklearn_cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES, ax=axes[0])\n",
    "axes[0].set_title('sklearn TF-IDF Confusion Matrix')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('True')\n",
    "\n",
    "# SPLADE confusion matrix\n",
    "splade_cm = confusion_matrix(test_labels, splade_preds)\n",
    "sns.heatmap(splade_cm, annot=True, fmt='d', cmap='Reds',\n",
    "            xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES, ax=axes[1])\n",
    "axes[1].set_title('SPLADE Confusion Matrix')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('True')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Interpretability Demo\n",
    "\n",
    "One of the key advantages of SPLADE is its inherent interpretability. Each dimension in the sparse vector corresponds to a vocabulary token, making it easy to understand *why* the model made a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example texts for interpretation\n",
    "example_texts = [\n",
    "    \"Apple stock surged 5% after announcing record iPhone sales and strong quarterly earnings.\",\n",
    "    \"The Lakers defeated the Celtics 112-98 in an exciting NBA playoff game last night.\",\n",
    "    \"Scientists discovered a new exoplanet that could potentially support life in a distant solar system.\",\n",
    "    \"Political tensions rise as world leaders meet at the United Nations summit to discuss climate change.\"\n",
    "]\n",
    "\n",
    "expected_classes = [\"Business\", \"Sports\", \"Sci/Tech\", \"World\"]\n",
    "\n",
    "print(\"SPLADE Interpretability Demo\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, text in enumerate(example_texts):\n",
    "    print(f\"\\nExample {i+1} (Expected: {expected_classes[i]})\")\n",
    "    splade_clf.print_explanation(text, top_k=10)\n",
    "    print(\"\\n\" + \"-\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compare with sklearn TF-IDF weights for interpretation\nprint(\"TF-IDF Top Terms (for comparison)\\n\")\n\n# Handle both old and new sklearn API\ntry:\n    feature_names = vectorizer.get_feature_names_out()\nexcept AttributeError:\n    feature_names = vectorizer.get_feature_names()\n\nfor i, text in enumerate(example_texts[:2]):  # Just first 2 examples\n    print(f\"Example {i+1}: {text[:80]}...\")\n    \n    # Get TF-IDF vector\n    tfidf_vec = vectorizer.transform([text])\n    \n    # Get top terms\n    indices = tfidf_vec.toarray()[0].argsort()[-10:][::-1]\n    \n    print(\"Top TF-IDF terms:\")\n    for idx in indices:\n        weight = tfidf_vec[0, idx]\n        if weight > 0:\n            print(f\"  {feature_names[idx]:<20} {weight:.3f}\")\n    print()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Sparsity Analysis\n",
    "\n",
    "Both methods produce sparse representations, but with different characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get SPLADE vectors for analysis\n",
    "sample_size = 100\n",
    "sample_texts_analysis = test_texts[:sample_size]\n",
    "\n",
    "# SPLADE vectors\n",
    "splade_vectors = splade_clf.transform(sample_texts_analysis)\n",
    "splade_nnz = (splade_vectors != 0).sum(dim=1).float()\n",
    "\n",
    "# TF-IDF vectors\n",
    "tfidf_vectors = vectorizer.transform(sample_texts_analysis)\n",
    "tfidf_nnz = np.array([(tfidf_vectors[i] != 0).sum() for i in range(sample_size)])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Non-zero elements histogram\n",
    "axes[0].hist(tfidf_nnz, bins=30, alpha=0.7, label='TF-IDF', color='#3498db')\n",
    "axes[0].hist(splade_nnz.numpy(), bins=30, alpha=0.7, label='SPLADE', color='#e74c3c')\n",
    "axes[0].set_xlabel('Number of non-zero elements')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Non-zero Elements per Document')\n",
    "axes[0].legend()\n",
    "\n",
    "# Sparsity comparison\n",
    "tfidf_total = tfidf_vectors.shape[1]\n",
    "splade_total = splade_vectors.shape[1]\n",
    "\n",
    "tfidf_sparsity = (1 - tfidf_nnz.mean() / tfidf_total) * 100\n",
    "splade_sparsity_val = (1 - splade_nnz.mean().item() / splade_total) * 100\n",
    "\n",
    "axes[1].bar(['TF-IDF', 'SPLADE'], [tfidf_sparsity, splade_sparsity_val], \n",
    "            color=['#3498db', '#e74c3c'])\n",
    "axes[1].set_ylabel('Sparsity (%)')\n",
    "axes[1].set_title('Vector Sparsity Comparison')\n",
    "axes[1].set_ylim(90, 100)\n",
    "for i, v in enumerate([tfidf_sparsity, splade_sparsity_val]):\n",
    "    axes[1].text(i, v + 0.3, f'{v:.2f}%', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nAverage non-zero elements:\")\n",
    "print(f\"  TF-IDF: {tfidf_nnz.mean():.1f} / {tfidf_total} ({tfidf_sparsity:.2f}% sparse)\")\n",
    "print(f\"  SPLADE: {splade_nnz.mean().item():.1f} / {splade_total} ({splade_sparsity_val:.2f}% sparse)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary & Conclusions\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Accuracy**: Both methods achieve strong performance on AG News\n",
    "2. **Training Time**: sklearn is faster (CPU-based, no backprop), SPLADE requires GPU training\n",
    "3. **Sparsity**: SPLADE produces sparser vectors (better for retrieval)\n",
    "4. **Interpretability**: SPLADE provides semantic term weights, TF-IDF provides lexical weights\n",
    "\n",
    "### When to use each:\n",
    "\n",
    "- **sklearn TF-IDF**: Quick prototyping, limited compute, large-scale batch processing\n",
    "- **SPLADE**: When interpretability matters, semantic matching needed, or for retrieval tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the SPLADE model\n",
    "splade_clf.save(\"../models/splade_ag_news.pt\")\n",
    "print(\"Model saved to ../models/splade_ag_news.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}