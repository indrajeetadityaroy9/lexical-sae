Metadata-Version: 2.4
Name: splade-classifier
Version: 0.1.0
Summary: SPLADE neural sparse text classifier with sklearn-compatible API
Requires-Python: >=3.9
Description-Content-Type: text/markdown

# SPLADE Classifier
A fast, interpretable sparse text classifier with an sklearn-compatible API. Built on [SPLADE](https://arxiv.org/abs/2107.05720) (Sparse Lexical and Expansion Model), this library provides neural sparse representations that outperform traditional TF-IDF while remaining fully interpretable.

## Features

- **Sklearn-compatible API** - `fit()`, `predict()`, `transform()` just like scikit-learn
- **Interpretable** - See exactly which terms drive each prediction
- **Sparse & Fast** - 98%+ sparsity with Triton-accelerated inference (10x speedup)
- **State-of-the-art** - Outperforms TF-IDF baselines by 3-4 percentage points

## Quick Start

```python
from src.models import SPLADEClassifier

# Train
clf = SPLADEClassifier()
clf.fit(train_texts, train_labels, epochs=5)

# Predict
predictions = clf.predict(test_texts)

# Explain predictions
clf.print_explanation("This movie was fantastic!")
```

## Benchmark Results

| Model | Accuracy | F1 Score | Sparsity |
|-------|----------|----------|----------|
| **SPLADE (Ours)** | **82.0%** | **0.829** | 98.3% |
| TF-IDF + LogReg | 78.5% | 0.805 | 95.2% |

## API Reference

### SPLADEClassifier

```python
clf = SPLADEClassifier(
    model_name="distilbert-base-uncased",  # Backbone model
    max_length=128,                         # Max sequence length
    batch_size=32,                          # Training batch size
    learning_rate=2e-5,                     # Learning rate
    flops_lambda=1e-4,                      # Sparsity regularization
    random_state=42,                        # Reproducibility seed
)
```

#### Methods

| Method | Description |
|--------|-------------|
| `fit(X, y, epochs=5)` | Train on texts and labels |
| `predict(X)` | Get class predictions |
| `predict_proba(X)` | Get class probabilities |
| `transform(X)` | Get sparse SPLADE vectors |
| `score(X, y)` | Compute accuracy |
| `save(path)` / `load(path)` | Model persistence |

#### Interpretability

| Method | Description |
|--------|-------------|
| `explain(text)` | Get top weighted terms |
| `explain_prediction(text)` | Full prediction breakdown |
| `compare_texts(text1, text2)` | Compare representations |
| `print_explanation(text)` | Pretty-print explanation |

## How It Works

SPLADE uses a masked language model (DistilBERT) to produce sparse vocabulary-sized vectors:

1. **Encode**: Text → DistilBERT → Token logits `[batch, seq_len, vocab_size]`
2. **Activate**: `log(1 + ReLU(logits))` for log-saturation
3. **Pool**: Max-pool over sequence → Sparse document vector
4. **Classify**: Linear layer on sparse vector

The resulting vectors are:
- **Sparse**: ~98% zeros (efficient storage/retrieval)
- **Interpretable**: Each dimension = vocabulary term weight
- **Expandable**: Semantically related terms get non-zero weights

## Advanced: Sparse Autoencoder Analysis

For deeper interpretability, train a Sparse Autoencoder on SPLADE vectors:

```bash
# Extract vectors
python -m src.extract_vectors --model_path models/model.pth

# Train SAE
python -m src.train_sae --vectors_path outputs/vectors.pt --epochs 10

# Analyze features
python -m src.analyze_sae --sae_path outputs/sae/sae_best.pt
```

This decomposes polysemantic SPLADE dimensions into monosemantic features.

## Performance Optimization

Triton kernels provide up to **10x speedup** on GPU inference:

| Operation | PyTorch | Triton | Speedup |
|-----------|---------|--------|---------|
| SPLADE Aggregation | 1.29ms | 0.12ms | **10.5x** |
| Log Saturation | 0.66ms | 0.33ms | 2.0x |

Triton is used automatically during inference when available.

